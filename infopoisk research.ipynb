{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17fd6f4",
   "metadata": {},
   "source": [
    "# Выбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691c48f",
   "metadata": {},
   "source": [
    "Создадим корпус из переписок чата дубков. Это может быть полезно для поиска информации, связанной с общежитием: когда работает кастелянная, как забронировать досуговую и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6550c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79998\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Union, Tuple\n",
    "import inspect\n",
    "import wget\n",
    "import zipfile\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b10866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./infopoisk HW1/dubki_res.json\", 'r', encoding = 'utf-8') as file:\n",
    "    data_dubki = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8351f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646346"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dubki['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c2a98",
   "metadata": {},
   "source": [
    "Возьмем часть записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d01292",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = data_dubki['messages'][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4164fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'type': 'service',\n",
       " 'date': '2020-09-07T23:31:35',\n",
       " 'date_unixtime': '1599510695',\n",
       " 'actor': 'Студенческий городок Дубки',\n",
       " 'actor_id': 'channel1278030013',\n",
       " 'action': 'migrate_from_group',\n",
       " 'title': 'Дубки',\n",
       " 'text': '',\n",
       " 'text_entities': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a1724",
   "metadata": {},
   "source": [
    "Возьмем только те, где есть текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2492e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_messages = []\n",
    "for m in messages:\n",
    "    if m['text']:\n",
    "        text = m['text']\n",
    "        if not isinstance(text, list): #берем только без вложений\n",
    "            text_messages.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8867810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4148"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312563a",
   "metadata": {},
   "source": [
    "Создадим таблицу, в которой будем хранить исходный текст, обработанный текст и два индекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8e0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['texts', 'lemmas_lists', 'lemmas_texts', 'index_w2v', 'index_tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b423bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = text_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10ae664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>lemmas_lists</th>\n",
       "      <th>lemmas_texts</th>\n",
       "      <th>index_w2v</th>\n",
       "      <th>index_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Я верю в развитие этого чата</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Привет</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Есть тусяо?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Народ, у кого-нибудь есть колонка проводная? З...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Шалом православным</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts lemmas_lists  \\\n",
       "0                       Я верю в развитие этого чата          NaN   \n",
       "1                                             Привет          NaN   \n",
       "2                                        Есть тусяо?          NaN   \n",
       "3  Народ, у кого-нибудь есть колонка проводная? З...          NaN   \n",
       "4                                 Шалом православным          NaN   \n",
       "\n",
       "  lemmas_texts index_w2v index_tfidf  \n",
       "0          NaN       NaN         NaN  \n",
       "1          NaN       NaN         NaN  \n",
       "2          NaN       NaN         NaN  \n",
       "3          NaN       NaN         NaN  \n",
       "4          NaN       NaN         NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cb70a",
   "metadata": {},
   "source": [
    "# Предобработка\n",
    "\n",
    "Далее \n",
    "* лемматизируем \n",
    "* уберем стоп-слова\n",
    "* уберем пунктуацию\n",
    "\n",
    "Это необходимо для дальнейшей векторизации так как\n",
    "1. ускоряет работу tf-idf (не надо делать то же во время подсчета)\n",
    "2. делает обработку с помощью word2vec осмысленной, так как мы выучиваем эмбеддинги только отедльных и значимых слов "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c6f36",
   "metadata": {},
   "source": [
    "Для лемматизации русского текста лучше всего подойдет natsha - хорошо работает для русского и занимает не много места, что будет важно при создании веб-приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41448128",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "sw = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66680806",
   "metadata": {},
   "source": [
    "Хранить предобработанные тексты нужно в двух видах: лист лемм с частью речи для W2V и леммы через пробел для tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6b53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Предобрабатывает входной текст, выполняя токенизацию, приведение к нижнему регистру\n",
    "    и фильтрацию стоп-слов и неалфавитных и нечисловых токенов.\n",
    "   \n",
    "    Args:\n",
    "        text (str):  Входной текст для предобработки.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, str]]:  Лист, содержащий леммы и части их речи. (lemmas).\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "        token_str = str(token.lemma).lower()\n",
    "        if token_str.isalpha() or token_str.isnumeric():  # числовые токены включаются\n",
    "            if token_str not in sw:\n",
    "                lemmas.append((token_str, token.pos))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "537e7bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('это', 'PRON'),\n",
       " ('мистический', 'ADJ'),\n",
       " ('выброс', 'NOUN'),\n",
       " ('весь', 'DET'),\n",
       " ('след', 'NOUN'),\n",
       " ('неделя', 'NOUN'),\n",
       " ('собираться', 'VERB'),\n",
       " ('против', 'ADP'),\n",
       " ('надоесть', 'VERB'),\n",
       " ('нло', 'NOUN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text_messages[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efedb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pos_and_text(preprocessed_lemmas: List[Tuple[str, str]]) -> Union[Tuple[List[List[str]], List[str]], None]:\n",
    "    \"\"\"\n",
    "    Преобразует лемматизированные и размеченные по чати речи тексты в кортеж из двух спиков, содержащих данные для удобной векторизации\n",
    "   \n",
    "    Args:\n",
    "       preprocessed_lemmas (List[Tuple[str, str]]):  Предобработанные тексты в виде лемм с частями речи.\n",
    "    \n",
    "    Returns:\n",
    "        List[List[str]]:  Кортеж листов, удобных для работы с Word2vec и TF-IDF.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmas_list_pos = [str(lemma[0]) + '_' + str(lemma[1]) for lemma in preprocessed_lemmas]\n",
    "    lemmas_text = ' '.join([str(lemma[0]) for lemma in preprocessed_lemmas])\n",
    "            \n",
    "    return (lemmas_list_pos, lemmas_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fe0f149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4148/4148 [00:09<00:00, 457.71it/s]\n"
     ]
    }
   ],
   "source": [
    "lemma_lists_pos = []\n",
    "lemma_texts = []\n",
    "\n",
    "for text in tqdm(text_messages):\n",
    "    preprocessed_lemmas = preprocess_pos_and_text(preprocess(text))\n",
    "    if preprocessed_lemmas:\n",
    "        lemma_lists_pos.append(preprocessed_lemmas[0])\n",
    "        lemma_texts.append(preprocessed_lemmas[1])\n",
    "    else:\n",
    "        lemma_lists_pos.append(None)\n",
    "        lemma_texts.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b9164f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['это_PRON',\n",
       " 'мистический_ADJ',\n",
       " 'выброс_NOUN',\n",
       " 'весь_DET',\n",
       " 'след_NOUN',\n",
       " 'неделя_NOUN',\n",
       " 'собираться_VERB',\n",
       " 'против_ADP',\n",
       " 'надоесть_VERB',\n",
       " 'нло_NOUN']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_lists_pos[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b35f525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'это мистический выброс весь след неделя собираться против надоесть нло'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_texts[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a73bc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmas_texts'] = lemma_texts\n",
    "data['lemmas_lists'] = lemma_lists_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fda316e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['lemmas_texts', 'lemmas_lists'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279430bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4148, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329ba5d",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "030b3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'http://vectors.nlpl.eu/repository/20/180.zip' # нкря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "689ed0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.5 s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m = wget.download(model_url)\n",
    "model_file = model_url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "110bef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.89 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23c77d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc: List[str], model: Word2Vec) -> Union[np.ndarray, np.array]:\n",
    "    \"\"\"\n",
    "    Преобразует документ в вектор с помощью усреднения векторов слов из модели Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        doc (List[str]): Список токенов (слов) документа.\n",
    "        model (Word2Vec): Предварительно обученная модель Word2Vec, из которой извлекаются векторы слов.\n",
    "\n",
    "    Returns:\n",
    "        Union[np.ndarray, np.array]: Усреднённый вектор документа. Если все слова отсутствуют в модели, возвращается нулевой вектор.\n",
    "    \"\"\"\n",
    "    word_vectors = [model[word] for word in doc if word in model]\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b517374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_doc_vec = [list(document_vector(text, model)) for text in data['lemmas_lists']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['index_w2v'] = w2v_doc_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf843c",
   "metadata": {},
   "source": [
    "В дальнейшем можно так же обрабатывать и получать из модели вектор, и сравнивать его с помощью косинусной близости"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7e14a",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ae5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3401a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vectorizer.fit_transform(data['lemmas_texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0025f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4148, 5284)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb8b4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "npm_tfidf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ca1c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vecs = [list(vec) for vec in npm_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f76f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['index_tfidf'] = tfidf_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea9506e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>lemmas_lists</th>\n",
       "      <th>lemmas_texts</th>\n",
       "      <th>index_w2v</th>\n",
       "      <th>index_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Я верю в развитие этого чата</td>\n",
       "      <td>[верить_VERB, развитие_NOUN, чат_NOUN]</td>\n",
       "      <td>верить развитие чат</td>\n",
       "      <td>[0.10236853, -0.441486, 2.36323, -0.5347944, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Привет</td>\n",
       "      <td>[привет_X]</td>\n",
       "      <td>привет</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Есть тусяо?</td>\n",
       "      <td>[тусяо_NOUN]</td>\n",
       "      <td>тусяо</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Народ, у кого-нибудь есть колонка проводная? З...</td>\n",
       "      <td>[народ_NOUN, колонка_NOUN, проводная_NOUN, шок...</td>\n",
       "      <td>народ колонка проводная шоколадка</td>\n",
       "      <td>[1.7437725, 0.825912, 0.62153894, -0.6994569, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Шалом православным</td>\n",
       "      <td>[шалый_PROPN, православный_ADJ]</td>\n",
       "      <td>шалый православный</td>\n",
       "      <td>[-0.8455712, 0.7992621, 0.2846047, -1.3858058,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  \\\n",
       "0                       Я верю в развитие этого чата   \n",
       "1                                             Привет   \n",
       "2                                        Есть тусяо?   \n",
       "3  Народ, у кого-нибудь есть колонка проводная? З...   \n",
       "4                                 Шалом православным   \n",
       "\n",
       "                                        lemmas_lists  \\\n",
       "0             [верить_VERB, развитие_NOUN, чат_NOUN]   \n",
       "1                                         [привет_X]   \n",
       "2                                       [тусяо_NOUN]   \n",
       "3  [народ_NOUN, колонка_NOUN, проводная_NOUN, шок...   \n",
       "4                    [шалый_PROPN, православный_ADJ]   \n",
       "\n",
       "                        lemmas_texts  \\\n",
       "0                верить развитие чат   \n",
       "1                             привет   \n",
       "2                              тусяо   \n",
       "3  народ колонка проводная шоколадка   \n",
       "4                 шалый православный   \n",
       "\n",
       "                                           index_w2v  \\\n",
       "0  [0.10236853, -0.441486, 2.36323, -0.5347944, 0...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [1.7437725, 0.825912, 0.62153894, -0.6994569, ...   \n",
       "4  [-0.8455712, 0.7992621, 0.2846047, -1.3858058,...   \n",
       "\n",
       "                                         index_tfidf  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff474e8",
   "metadata": {},
   "source": [
    "# Облегчение поиска: нормализация векторов заранее\n",
    "Запишем норму векторов, чтобы при подсчете косинусной близости надо было делать меньше операций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e934fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vec):\n",
    "    return np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80ce1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.25 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['w2v_norm'] = [normalize(vec) for vec in data['index_w2v']]\n",
    "data['tfidf_norm'] = [normalize(vec) for vec in data['index_tfidf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf72f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество документов с нулевой нормой Word2Vec: 702\n",
      "Количество документов с нулевой нормой TF-IDF: 312\n"
     ]
    }
   ],
   "source": [
    "# Проверка количества документов с нулевой нормой\n",
    "zero_norm_w2v = data[data['w2v_norm'] == 0].shape[0]\n",
    "print(f'Количество документов с нулевой нормой Word2Vec: {zero_norm_w2v}')\n",
    "\n",
    "zero_norm_tfidf = data[data['tfidf_norm'] == 0].shape[0]\n",
    "print(f'Количество документов с нулевой нормой TF-IDF: {zero_norm_tfidf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6be67b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документы с нулевой нормой Word2Vec:\n",
      "1          Привет\n",
      "2     Есть тусяо?\n",
      "13              ?\n",
      "52          Движ?\n",
      "53          Гдеее\n",
      "Name: texts, dtype: object\n"
     ]
    }
   ],
   "source": [
    "zero_norm_docs_w2v = data[data['w2v_norm'] == 0]['texts'].head()\n",
    "print('Документы с нулевой нормой Word2Vec:')\n",
    "print(zero_norm_docs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd0bc1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документы с нулевой нормой tfidf:\n",
      "13        ?\n",
      "93       да\n",
      "100      ❤️\n",
      "123    Есть\n",
      "125     ???\n",
      "Name: texts, dtype: object\n"
     ]
    }
   ],
   "source": [
    "zero_norm_docs_tfidf = data[data['tfidf_norm'] == 0]['texts'].head()\n",
    "print('Документы с нулевой нормой tfidf:')\n",
    "print(zero_norm_docs_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cd27d",
   "metadata": {},
   "source": [
    "Из-за того, что некоторые сообщения состоят из одного слова, которые W2V не знает, или просто из символов, и вектор, и норма равны нулю. Чтобы избежать деления на 0 при подсчете косинусной близости, нам приедтся выкинуть эти записи. Их довольно много, но они все равно не несут много смысла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b7a8540",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['w2v_norm'] != 0].reset_index(drop=True)\n",
    "data = data[data['tfidf_norm'] != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e5db97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3446, 7)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544625dc",
   "metadata": {},
   "source": [
    "#  Поиск\n",
    "\n",
    "реализуем функции поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a877b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2, norm1, norm2):\n",
    "    scalar_mul = np.dot(vec1, vec2)\n",
    "    return scalar_mul / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a018bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, search_type: str, top_n: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Выполняет поиск наиболее похожих текстов в базе данных на основе заданного запроса и метода поиска.\n",
    "\n",
    "    Args:\n",
    "        query (str): Входной текст запроса, который нужно искать.\n",
    "        search_type (str): Тип поиска. Возможные значения:\n",
    "            - 'w2v' для поиска на основе word2vec векторов.\n",
    "            - 'tfidf' для поиска на основе TF-IDF.\n",
    "        top_n (int): Количество наиболее похожих текстов, которые следует вернуть.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Серия из топ-N текстов, наиболее похожих на запрос.\n",
    "\n",
    "    Этапы:\n",
    "    1. Предобрабатывает запрос с использованием функции `preprocess_pos_and_text`.\n",
    "    2. В зависимости от выбранного типа поиска ('w2v' или 'tfidf') извлекает вектор запроса.\n",
    "    3. Нормализует вектор запроса.\n",
    "    4. Вычисляет косинусное сходство между вектором запроса и векторами в базе данных.\n",
    "    5. Возвращает топ-N текстов с наибольшим сходством.\n",
    "    \"\"\"\n",
    "    preprocessed_q = preprocess_pos_and_text(preprocess(query))\n",
    "    \n",
    "    if search_type == 'w2v':\n",
    "        preprocessed_q = preprocessed_q[0]\n",
    "        q_vector = document_vector(preprocessed_q, model)\n",
    "        index_name = 'index_w2v'\n",
    "        norm_name = 'w2v_norm'\n",
    "        \n",
    "    elif search_type == 'tfidf':\n",
    "        preprocessed_q = preprocessed_q[1]\n",
    "        q_vector = vectorizer.transform([preprocessed_q]).toarray()[0]\n",
    "        index_name = 'index_tfidf'\n",
    "        norm_name = 'tfidf_norm'\n",
    "        \n",
    "    q_norm = normalize(q_vector)\n",
    "    \n",
    "    similarities = []\n",
    "    for idx, vec, norm in zip(data.index, data[index_name], data[norm_name]):\n",
    "        sim = cosine_sim(q_vector, vec, q_norm, norm)\n",
    "        similarities.append((sim, idx))\n",
    "        \n",
    "    top_similarities = heapq.nlargest(top_n, similarities, key=lambda x: x[0])\n",
    "    top_idxs = [idx for _, idx in top_similarities]    \n",
    "    top_texts = data.loc[top_idxs, 'texts']\n",
    "    return top_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e74ec9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Где достать еду в дубках'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27a59c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ладно хоть еду приносят\n",
      "_____\n",
      "Через вк еда много доставок есть\n",
      "_____\n",
      "Съем свои купленные чипсы.\n",
      "_____\n",
      "Я за супом то не пошел, а тут ложка сметаны\n",
      "_____\n",
      "ну засунь в морозилку\n",
      "_____\n",
      "CPU times: total: 594 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = search(query, 'w2v', 5)\n",
    "for i in res:\n",
    "    print(i)\n",
    "    print('_____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5244bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Еду с КГ\n",
      "_____\n",
      "Через вк еда много доставок есть\n",
      "_____\n",
      "Андрей достал - прими гастал\n",
      "_____\n",
      "Куда пойти в дубках?\n",
      "_____\n",
      "Ладно хоть еду приносят\n",
      "_____\n",
      "CPU times: total: 4.52 s\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = search(query, 'tfidf', 5)\n",
    "for i in res:\n",
    "    print(i)\n",
    "    print('_____')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcb85b",
   "metadata": {},
   "source": [
    "Итог: поиск работает очень быстро и достаточно информативно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76e57b",
   "metadata": {},
   "source": [
    "# Сохраняем для дальнейшего использования\n",
    "* функции предобработки\n",
    "* функции нормализации и подсчета косинусной близости\n",
    "* модель w2v и функцию векторизации\n",
    "* векторайзер tf-idf\n",
    "* полученную табличку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a786c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем функции предобработки\n",
    "preprocess_code = inspect.getsource(preprocess)\n",
    "preprocess_pos_and_text_code = inspect.getsource(preprocess_pos_and_text)\n",
    "\n",
    "with open(\"./infopoisk HW1/preprocess_functions.py\", 'w', encoding = 'utf-8') as file:\n",
    "    file.write(preprocess_code +'\\n\\n')    \n",
    "    file.write(preprocess_pos_and_text_code +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a5db094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем функции нормы и косинусной близости\n",
    "normalize_code = inspect.getsource(normalize)\n",
    "cosine_sim_code = inspect.getsource(cosine_sim)\n",
    "\n",
    "with open(\"./infopoisk HW1/math_functions.py\", 'w', encoding = 'utf-8') as file:\n",
    "    file.write(normalize_code +'\\n\\n')    \n",
    "    file.write(cosine_sim_code +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed9beca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем функцию векторизации текста с помощью W2V\n",
    "docvec_code = inspect.getsource(document_vector)\n",
    "\n",
    "with open(\"./infopoisk HW1/docvec_function.py\", 'w', encoding = 'utf-8') as file:\n",
    "    file.write(docvec_code +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a1cfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем функцию поиска\n",
    "search_code = inspect.getsource(search)\n",
    "\n",
    "with open(\"./infopoisk HW1/search_function.py\", 'w', encoding = 'utf-8') as file:\n",
    "    file.write(search_code +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76f2bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем векторайзер\n",
    "with open('./infopoisk HW1/tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9752d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохраняем модель w2v\n",
    "model.save(\"./infopoisk HW1/w2v_model.bin\")  # Сохранение в KeyedVectors формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32fcc31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.56 s\n",
      "Wall time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#сохраняем все данные\n",
    "data.to_csv('./infopoisk HW1/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2455f6",
   "metadata": {},
   "source": [
    "Теперь используем это для реализации сайта с поисковиком"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00de879",
   "metadata": {},
   "source": [
    "# Формальное сравнение двух индексаций\n",
    "\n",
    "Используя весь уже имеющийся инструментарий, можем сравнить, насколько хорошо модели/векторайзеры справляются с поиском как бы по собственному мнению, а именно, посмотреть на косинусные близости. Чем больше косинусная близость между запросом и top_n, тем круче работает поиск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc4812b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(query: str, top_n: int) -> pd.Series:\n",
    "    \n",
    "    preprocessed_q = preprocess_pos_and_text(preprocess(query))\n",
    "    \n",
    "    preprocessed_q_w2v = preprocessed_q[0]\n",
    "    q_vector_w2v = document_vector(preprocessed_q_w2v, model)\n",
    "    index_name_w2v = 'index_w2v'\n",
    "    norm_name_w2v = 'w2v_norm'\n",
    "        \n",
    "    preprocessed_q_tfidf = preprocessed_q[1]\n",
    "    q_vector_tfidf = vectorizer.transform([preprocessed_q_tfidf]).toarray()[0]\n",
    "    index_name_tfidf = 'index_tfidf'\n",
    "    norm_name_tfidf = 'tfidf_norm'\n",
    "    \n",
    "    q_norm_w2v = normalize(q_vector_w2v)   \n",
    "    q_norm_tfidf = normalize(q_vector_tfidf)\n",
    "        \n",
    "    similarities_w2v = []\n",
    "    similarities_tfidf = []\n",
    "    \n",
    "    # считаем сходства для w2v\n",
    "    for vec, norm in zip(data[index_name_w2v], data[norm_name_w2v]):\n",
    "        sim = cosine_sim(q_vector_w2v, vec, q_norm_w2v, norm)\n",
    "        similarities_w2v.append(sim)\n",
    "        \n",
    "    # считаем сходства для tf-idf    \n",
    "    for vec, norm in zip(data[index_name_tfidf], data[norm_name_tfidf]):\n",
    "        sim = cosine_sim(q_vector_tfidf, vec, q_norm_tfidf, norm)\n",
    "        similarities_tfidf.append(sim)\n",
    "        \n",
    "    top_similarities_w2v = heapq.nlargest(top_n, similarities_w2v, key=lambda x: x)\n",
    "    top_similarities_tfidf = heapq.nlargest(top_n, similarities_tfidf, key=lambda x: x)\n",
    "\n",
    "    return top_similarities_w2v, top_similarities_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f74883cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['Где найти еду',\n",
    "           'Почему дежурка приходит так часто',\n",
    "           'Как отсюда уехать',\n",
    "           'тусовки со студсоветом',\n",
    "          'почему небо голубое',\n",
    "          'расписание сессий НИУ ВШЭ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "966908a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Где найти еду\n",
      "0.676 \t 0.596\n",
      "0.609 \t 0.596\n",
      "0.585 \t 0.596\n",
      "0.585 \t 0.542\n",
      "0.585 \t 0.492\n",
      "- - - - - - - - - - - - - - - -\n",
      "Почему дежурка приходит так часто\n",
      "0.572 \t 0.578\n",
      "0.572 \t 0.452\n",
      "0.572 \t 0.399\n",
      "0.572 \t 0.398\n",
      "0.572 \t 0.398\n",
      "- - - - - - - - - - - - - - - -\n",
      "Как отсюда уехать\n",
      "0.727 \t 0.436\n",
      "0.589 \t 0.409\n",
      "0.578 \t 0.359\n",
      "0.511 \t 0.309\n",
      "0.473 \t 0.306\n",
      "- - - - - - - - - - - - - - - -\n",
      "тусовки со студсоветом\n",
      "1.0 \t 0.732\n",
      "1.0 \t 0.732\n",
      "0.614 \t 0.471\n",
      "0.61 \t 0.387\n",
      "0.581 \t 0.344\n",
      "- - - - - - - - - - - - - - - -\n",
      "почему небо голубое\n",
      "0.461 \t 0.627\n",
      "0.422 \t 0.54\n",
      "0.422 \t 0.54\n",
      "0.422 \t 0.54\n",
      "0.422 \t 0.54\n",
      "- - - - - - - - - - - - - - - -\n",
      "расписание сессий НИУ ВШЭ\n",
      "0.62 \t 0.557\n",
      "0.617 \t 0.321\n",
      "0.613 \t 0.308\n",
      "0.613 \t 0.295\n",
      "0.581 \t 0.295\n",
      "- - - - - - - - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "for q in queries: \n",
    "    print(q)\n",
    "    sims = get_similarities(q, 5)\n",
    "    for wv, tf in zip(sims[0], sims[1]):\n",
    "        print(round(wv, 3), '\\t', round(tf, 3))\n",
    "    print('- - - - - - - - - - - - - - - -')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5107ae1",
   "metadata": {},
   "source": [
    "На глаз кажется, что W2V в среднем справляется лучше\n",
    "\n",
    "Далее как cамый просто формального сравнения - вычесть попарно сходства и найти среднее. Получим условную величину того, насколько один индекс лучше другого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57d93ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Где найти еду\n",
      "0.077420120813861\n",
      "- - - - - - - - - - - - - - - - - - -\n",
      "Почему дежурка приходит так часто\n",
      "0.1451854333103196\n",
      "- - - - - - - - - - - - - - - - - - -\n",
      "Как отсюда уехать\n",
      "0.2772900666108932\n",
      "- - - - - - - - - - - - - - - - - - -\n",
      "тусовки со студсоветом\n",
      "0.28725473337038815\n",
      "- - - - - - - - - - - - - - - - - - -\n",
      "почему небо голубое\n",
      "-0.03953198423675864\n",
      "- - - - - - - - - - - - - - - - - - -\n",
      "расписание сессий НИУ ВШЭ\n",
      "0.2560921190297827\n",
      "- - - - - - - - - - - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "top_n = 15\n",
    "for q in queries: \n",
    "    print(q)\n",
    "    sims = get_similarities(q, top_n)\n",
    "    subs = []\n",
    "    for wv, tf in zip(sims[0], sims[1]):\n",
    "        subs.append(wv - tf)\n",
    "    print(sum(subs)/top_n)\n",
    "    print('- - - - - - - - - - - - - - - - - - -')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b85491",
   "metadata": {},
   "source": [
    "Большинство ответов положительные (хоть иногда и близкие к нулю), следовательно победу можно присудить Word2Vec. В дальнейшем для более точного анализа можно посмотреть на среднне аналогичной метрики по большому количеству запросов (например, сгенерированных нейросетью), делить эти запросы на категории (например, запросы связанные с ВШЭ, абстрактные запросы и т.п.), брать больше реpультатов и учитывать при вычислениях их порядок выдачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239731ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
